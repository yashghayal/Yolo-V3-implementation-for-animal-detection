{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolov3_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMltp/yM9N3mg07W7N3Nknc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcDPSQ9abC7r"
      },
      "source": [
        "from __future__ import division\n",
        "import time\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import cv2\n",
        "from utils.util import *\n",
        "from Darknet_VOC import Darknet\n",
        "import random\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wf5oRsWbMPZ"
      },
      "source": [
        "def write(x, img):\n",
        "\n",
        "    cls = int(x[-1])\n",
        "    label = \"{0}\".format(classes[cls])\n",
        "    c1 = tuple(x[1:3].int())\n",
        "    c2 = tuple(x[3:5].int())\n",
        "    color = random.choice(colors)\n",
        "    cv2.rectangle(img, c1, c2, color, 2)\n",
        "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
        "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
        "    cv2.rectangle(img, c1, c2, color, -1)\n",
        "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225, 255, 255], 1)\n",
        "\n",
        "    return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPa2iGLWbR5X"
      },
      "source": [
        "def arg_parse():\n",
        "    \"\"\"\n",
        "    Parse arguements to the detect module\n",
        "    \"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='YOLO v3 Demo')\n",
        "    parser.add_argument(\"--confidence\", dest=\"confidence\", help=\"Object Confidence to filter predictions\", default=0.5)\n",
        "    parser.add_argument(\"--nms_thresh\", dest=\"nms_thresh\", help=\"NMS Threshhold\", default=0.4)\n",
        "    parser.add_argument(\"--reso\", dest='reso', help=\n",
        "    \"Input resolution of the network. Increase to increase accuracy. Decrease to increase speed\",\n",
        "                        default=\"416\", type=str)\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    classes = load_classes(\"data/voc.names\")\n",
        "    colors = pkl.load(open(\"data/pallete\", \"rb\"))\n",
        "\n",
        "    args = arg_parse()\n",
        "    confidence = float(args.confidence)\n",
        "    nms_thesh = float(args.nms_thresh)\n",
        "    CUDA = torch.cuda.is_available()\n",
        "\n",
        "    num_classes = 20\n",
        "\n",
        "    model = Darknet()\n",
        "    print(\"Loading network.....\")\n",
        "    model.load_state_dict(torch.load('weights/Dartnet_VOC_Weights', map_location=lambda storage, loc: storage))\n",
        "    print(\"Network successfully loaded\")\n",
        "    model.image_size = args.reso\n",
        "    inp_dim = int(model.image_size)\n",
        "\n",
        "    assert inp_dim % 32 == 0\n",
        "    assert inp_dim > 32\n",
        "\n",
        "    if CUDA:\n",
        "        model.cuda()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    frame = cv2.imread('imgs/timg.jpeg')\n",
        "    img, orig_im = prep_image(frame, inp_dim)\n",
        "\n",
        "    if CUDA:\n",
        "        img = img.cuda()\n",
        "\n",
        "    output, _ , _, _= model(Variable(img), CUDA)\n",
        "    # batch_size x number of boxes x attrs (85)  attrs have been transposed to input image\n",
        "    output = write_results(output, confidence, num_classes, nms_conf=nms_thesh)\n",
        "    # D x 8, D is the true detection  8: image index in batch, 4 corner coordinates, object score,highest class score, class index\n",
        "\n",
        "    if isinstance(output, int) == False:\n",
        "        output[:, 1:5] = torch.clamp(output[:, 1:5], 0.0, float(inp_dim)) / inp_dim\n",
        "        output[:, [1, 3]] *= orig_im.shape[1]\n",
        "        output[:, [2, 4]] *= orig_im.shape[0]\n",
        "\n",
        "        list(map(lambda x: write(x, orig_im), output))\n",
        "\n",
        "    print(\"image predicted in {:2.3f} seconds\".format(time.time()- start))\n",
        "\n",
        "    cv2.imshow(\"frame\", orig_im)\n",
        "    cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}